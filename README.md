# E-Commerce ETL Pipeline

## Project Overview
This project implements a robust ETL pipeline in Python that ingests e-commerce data from a REST API and loads it into Snowflake for downstream analytics and reporting. The pipeline extracts data from three API resources—customers, orders, and order-line-items using a reusable pagination handler capable of retrieving datasets spanning multiple pages of 100 records each. Each dataset is then transformed into a Snowflake-ready structure: nested fields such as customer address objects are normalized, timestamps are preserved, and operational metadata fields (_loaded_at, _source) are appended to support lineage, auditing, and debugging. After transformation, records are written into the RAW.ECOMMERCE schema through the provided load_to_snowflake function, which is treated as an external dependency per the project requirements. Configuration is fully environment-driven, making the pipeline portable across development, staging, and production environments. Logging and structured exception handling ensure observability and controlled failure modes. Unit tests validate pagination logic, transformation correctness, and loading behavior through targeted mocking.

## Performance and Scalability Considerations

While the current pipeline is designed for clarity and reliability, it provides a strong foundation for scaling to higher data volumes and more complex operational requirements. For larger datasets or higher-frequency ingestion, Snowflake performance can be improved by transitioning from row-level inserts to bulk staged loads using `COPY INTO`, implementing `MERGE`-based idempotent upserts, and applying micro-partition or clustering key optimizations to enhance long-term query efficiency. The ingestion strategy can shift from full refresh to incremental processing by leveraging `updated_at` timestamps or CDC-based mechanisms where supported by the API. Pipeline resilience can be strengthened through retry logic with exponential backoff, request throttling, and API rate-limit awareness. Structured error handling—including error tables for rejected records—would improve observability and robustness. Additionally, integrating the pipeline with an orchestrator such as `Airflow`, `AWS Step Functions`, or `Dagster` would provide enterprise-grade scheduling, monitoring, alerting, and dependency management, enabling the workflow to operate reliably at scale.